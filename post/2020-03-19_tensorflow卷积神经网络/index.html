<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>TensorFlow卷积神经网络 - Even - A super concise theme for Hugo</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=cache-control content="no-transform"><meta http-equiv=cache-control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="zyuanlee"><meta name=description content="卷积神经网络简介 与传统多层神经网络对比 传统意义上的多层神经网络是只有输入层、隐藏层与输出层。其中隐藏层的层数根据需要而定，没有明确的理论推导"><meta name=keywords content="Hugo,theme,even"><meta name=generator content="Hugo 0.85.0 with theme even"><link rel=canonical href=http://localhost:1313/post/2020-03-19_tensorflow%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/manifest.json><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link href=/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous><meta property="og:title" content="TensorFlow卷积神经网络"><meta property="og:description" content="卷积神经网络简介 与传统多层神经网络对比 传统意义上的多层神经网络是只有输入层、隐藏层与输出层。其中隐藏层的层数根据需要而定，没有明确的理论推导"><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/post/2020-03-19_tensorflow%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"><meta property="article:section" content="post"><meta property="article:published_time" content="2020-03-19T19:13:27+00:00"><meta property="article:modified_time" content="2020-03-19T19:13:27+00:00"><meta itemprop=name content="TensorFlow卷积神经网络"><meta itemprop=description content="卷积神经网络简介 与传统多层神经网络对比 传统意义上的多层神经网络是只有输入层、隐藏层与输出层。其中隐藏层的层数根据需要而定，没有明确的理论推导"><meta itemprop=datePublished content="2020-03-19T19:13:27+00:00"><meta itemprop=dateModified content="2020-03-19T19:13:27+00:00"><meta itemprop=wordCount content="3521"><meta itemprop=keywords content="TensorFlow,深度学习,"><meta name=twitter:card content="summary"><meta name=twitter:title content="TensorFlow卷积神经网络"><meta name=twitter:description content="卷积神经网络简介 与传统多层神经网络对比 传统意义上的多层神经网络是只有输入层、隐藏层与输出层。其中隐藏层的层数根据需要而定，没有明确的理论推导"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>Blog</a></div><div class=mobile-navbar-icon><span></span><span></span><span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=/><li class=mobile-menu-item>Home</li></a><a href=/post/><li class=mobile-menu-item>Archives</li></a><a href=/tags/><li class=mobile-menu-item>Tags</li></a><a href=/categories/><li class=mobile-menu-item>Categories</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=/ class=logo>Blog</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/>Home</a></li><li class=menu-item><a class=menu-item-link href=/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=/categories/>Categories</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>TensorFlow卷积神经网络</h1><div class=post-meta><span class=post-time>2020-03-19</span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>Contents</h2><div class="post-toc-content always-active"><nav id=TableOfContents><ul><li><ul><li><a href=#卷积神经网络简介>卷积神经网络简介</a><ul><li><a href=#与传统多层神经网络对比>与传统多层神经网络对比</a></li></ul></li><li><a href=#卷积神经网络原理>卷积神经网络原理</a><ul><li><a href=#结构>结构</a></li><li><a href=#卷积层convolution-layer>卷积层（Convolution Layer）</a></li><li><a href=#卷积网络-api>卷积网络 API</a></li><li><a href=#激活函数>激活函数</a></li><li><a href=#池化层>池化层</a></li></ul></li><li><a href=#案例cnn-mnist-手写数字识别>案例：CNN-Mnist 手写数字识别</a><ul><li><a href=#网络设计>网络设计</a></li><li><a href=#调参提高准确率>调参：提高准确率</a></li><li><a href=#改为高级-api>改为高级 API</a></li><li><a href=#主要代码>主要代码</a></li></ul></li><li><a href=#实战验证码识别>实战：验证码识别</a><ul><li><a href=#数据集>数据集</a></li><li><a href=#损失衡量>损失衡量</a></li><li><a href=#流程分析>流程分析</a></li><li><a href=#代码>代码</a></li></ul></li></ul></li></ul></nav></div></div><div class=post-content><h2 id=卷积神经网络简介>卷积神经网络简介</h2><h3 id=与传统多层神经网络对比>与传统多层神经网络对比</h3><ul><li>传统意义上的多层神经网络是只有输入层、隐藏层与输出层。其中隐藏层的层数根据需要而定，没有明确的理论推导来说明到底多少层合适</li><li>卷积神经网络 CNN，在原来多层神经网络的基础上，加入了更加有效的特征学习部分，具体操作就是在原来的全连接层前面加入了卷积层与池化层。<strong>卷积神经网络出现，使得神经网络层数得以加深，“深度”学习由此而来</strong></li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-fallback data-lang=fallback>输入层
隐藏层
	卷积层
	激活层
	池化层
	全连接层
输出层
</code></pre></td></tr></table></div></div><blockquote><p>通常所说的深度学习，一般指的是这些 CNN 等新的结构以及一些新的方法（比如新的激活函数 Relu 等），解决了传统多层神经网路的一些难以解决的问题</p></blockquote><h2 id=卷积神经网络原理>卷积神经网络原理</h2><p>在隐藏层加入卷积层和池化层，激活层</p><h3 id=结构>结构</h3><ul><li>卷积层<ul><li>通过在原始图像上平移来提取特征</li></ul></li><li>激活层<ul><li>增加非线性分割能力</li></ul></li><li>池化层<ul><li>减少学习的参数，降低网络的复杂度（最大池化和平均池化）</li></ul></li><li>全连接层<ul><li>为了能够达到分类效果</li></ul></li></ul><h3 id=卷积层convolution-layer>卷积层（Convolution Layer）</h3><p>卷积神经网络中每层卷积层由若干卷积单元（卷积核）组成，每个卷积单元的参数都是通过反向传播算法最佳化得到的。</p><p>卷积运算的目的是特征提取，第一层卷积层可能只能提取一些低级的特征如，边缘、线条和角等层级，更多层的网络能从低级特征中迭代提取更复杂的特征</p><p>卷积核、filter、过滤器、模型参数、卷积单元，相同</p><h4 id=卷积核四大要素>卷积核四大要素</h4><ul><li>个数<ul><li>不同的卷积核带的权重和偏置都不一样，即随机初始化的参数</li></ul></li><li>大小<ul><li><code>1*1 3*3 5*5</code></li></ul></li><li>步长<ul><li>跳几格</li></ul></li><li>零填充大小</li></ul><p>卷积核可以理解为一个观察的人，带着若干权重和一个偏置去观察，进行特征加运算</p><p><img src=https://raw.githubusercontent.com/Pandalzy/cloud_img/master/img/blog20200314195926.png alt></p><blockquote><p>上述要加上偏置</p></blockquote><h4 id=输出大小计算公式>输出大小计算公式</h4><ul><li>输出体积大小<code>H1*W1*D1</code><ul><li>输入图像<code>32*32*1</code></li></ul></li><li>四个超参数<ul><li>filter 数量 K</li><li>filter 大小 F</li><li>步长 S</li><li>零填充大小 P</li></ul></li><li>输出体积大小<code>H2*W2*D2</code><ul><li><code>H2=(H1-F+2P)/S+1</code></li><li><code>W2=(W1-F+2P)/S+1</code></li><li><code>D2=K</code></li></ul></li></ul><h4 id=计算案例>计算案例</h4><ul><li><p>假设已知的条件：输入图像<code>32*32*1</code>，50 个 filter，大小为<code>5*5</code>，移动步长为 1，零填充为 1，请求出输出大小</p><ul><li><code>H2=(H1-F+2P)/S+1=(32-5+2)/1+1=30</code></li><li><code>W2=(H1-F+2P)/S+1=(32-5+2)/1+1=30</code></li><li><code>D2=K=50</code></li><li><code>[30, 30, 50]</code></li></ul></li><li><p>假设已知的条件：输入图像<code>32*32*1</code>，50 个 filter，大小为<code>3*3</code>，移动步长为 1，输出大小<code>32*32</code>，求零填充</p></li></ul><h4 id=多通道图片观察>多通道图片观察</h4><h3 id=卷积网络-api>卷积网络 API</h3><ul><li><code>tf.nn.conv2d(input, filter, strides, padding, name)</code><ul><li>计算给定 4-D input 和 filter 张量的 2 维卷积</li><li>input：给定的输入张量，具有<code>[batch, height, width, channel]</code>，类型为 float32, 64<ul><li>batch：批数量</li></ul></li><li>filter：指定过滤器的权重数量<ul><li><code>[filter_height, filter_width, in_channels, out_channels]</code><ul><li>[卷积核的高度，卷积核的宽度，图像通道数，卷积核个数]</li><li>第三维<code>in_channels</code>，就是参数 input 的第四维</li></ul></li><li>变量，<code>initial_value=random_normal(shape=[F, F, 3/1, K])</code></li></ul></li><li>strides：<code>strides = [1, stride, stride, 1]</code>，步长</li><li>padding：“SAME”，“VAKID”</li></ul></li><li>零填充的两种方式<ul><li>SAME：越过边缘取样，取样的面积和输入图像的像素宽度一致<ul><li>公式：<code>ceil(H/S)</code><ul><li>H 为输入图片的高或者宽，S 为步长</li><li>无论过滤器的大小是多少，零填充的数量由 API 计算</li></ul></li></ul></li><li>VALID：不越过边缘取样，取样的面积小于输入人的图像的像素宽度，不填充</li></ul></li></ul><blockquote><p>在 Tensorflow 中，卷积 API 设置为“SAME”之后，如果步长为 1，输出高度与输入大小一样</p></blockquote><h3 id=激活函数>激活函数</h3><h4 id=relu>Relu</h4><p>$$
relu=max(0,x)
$$</p><p>x 小于 0，值为 0，大于 0，为其本身</p><h4 id=为什么采用新的激活函数>为什么采用新的激活函数</h4><ul><li>Relu 优点<ul><li>有效解决梯度消失问题</li><li>计算速度非常快，只需要判断输入是否大于 0，SGD（批梯度下降）的求解速度远快于 sigmoid 和 tanh</li></ul></li><li>sigmoid 缺点<ul><li>计算量相对较大，在深层网络中，sigmoid 函数反向传播时，很容易就会出现梯度消失的情况</li></ul></li></ul><h4 id=激活函数-api>激活函数 API</h4><ul><li><code>tf.nn.relu(features, name=None)</code><ul><li>features：卷积后加上偏置的结果</li><li>return：结果</li></ul></li></ul><h3 id=池化层>池化层</h3><p>Pooling 层主要的作用是特征提取，通过去掉 Feature Map 中不重要的样本，进一步减少参数的数量。方法有很多，通常采用最大池化</p><ul><li>max_polling：取池化窗口的最大值</li><li>avg_poling：取池化窗口的平均值</li></ul><h4 id=池化层-api>池化层 API</h4><ul><li><code>tf.nn.max_pool(value, ksize, strides, padding, name)</code><ul><li>输入上执行最大池数</li><li>value：4-D Tensor 形状<code>[batch, height, width, channel]</code></li><li>channel：不是原始图片的通道数，而是多少 filter 观察</li><li>ksize：池化窗口大小，<code>[1, ksize, ksize, 1]</code></li><li>strides：步长大小，<code>[1, strides, strides, 1]</code></li><li>padding：使用填充算法类型，“SAME”，“VAKID”</li></ul></li></ul><blockquote><p>卷积向下取整，池化向上取整</p></blockquote><h2 id=案例cnn-mnist-手写数字识别>案例：CNN-Mnist 手写数字识别</h2><h3 id=网络设计>网络设计</h3><ul><li>第一个卷积大层<ul><li>卷积层：32 个 filter、大小 5*5、strides=1、padding=&lsquo;SAME&rsquo;<ul><li><code>tf.nn.conv2d(input, filter, strides, padding'SAME')</code></li><li>input<ul><li><code>[None, 28, 28, 1]</code></li></ul></li><li>filter<ul><li><code>weights = tf.Variable(initial_value=tf.random_normal(shape=[5, 5, 1, 32]))</code></li><li><code>bias = tf.Variable(initial_value=tf.random_normal(shape=[32]))</code></li></ul></li><li>strides<ul><li><code>[1, 1, 1, 1]</code></li></ul></li><li>输出形状：<code>[None, 28, 28, 32]</code></li></ul></li><li>激活层：Relu<ul><li><code>tf.nn.relu(features)</code></li></ul></li><li>池化层：大小<code>2*2</code>、strides=2<ul><li><code>tf.nn.max_pool()</code></li><li>输入形状：<code>[None, 28, 28, 32]</code></li><li>输出形状：<code>[None, 14, 14, 32]</code>（公式计算）</li></ul></li></ul></li><li>第二个卷积大层<ul><li>卷积层：64 个 filter、大小 5*5、strides=1、padding=&lsquo;SAME&rsquo;<ul><li><code>tf.nn.conv2d(input, filter, strides, padding'SAME')</code></li><li>input<ul><li><code>[None, 14, 14, 32]</code></li></ul></li><li>filter<ul><li><code>weights = tf.Variable(initial_value=tf.random_normal(shape=[5, 5, 32, 64]))</code></li><li><code>bias = tf.Variable(initial_value=tf.random_normal(shape=[64]))</code></li></ul></li><li>strides<ul><li><code>[1, 1, 1, 1]</code></li></ul></li><li>输出形状：<code>[None, 14, 14, 64]</code></li></ul></li><li>激活层：Relu<ul><li><code>tf.nn.relu(features)</code></li></ul></li><li>池化层：大小<code>2*2</code>、strides=2<ul><li>输入形状：<code>[None, 14, 14, 64]</code></li><li>输出形状：<code>[None, 7, 7, 64]</code>（公式计算）</li></ul></li></ul></li><li>全连接层<ul><li><code>tf.reshape()</code></li><li><code>[None, 7, 7, 64]->[None, 7*7*64]</code></li><li><code>[None, 7*7*64] * [7*7*64, 10] = [None, 10]</code></li><li><code>y_predict = tf.matmul(pool2, weights) + bias</code></li></ul></li></ul><h3 id=调参提高准确率>调参：提高准确率</h3><ul><li>学习率，一般：0.01</li><li>随机初始化的权重、偏置的值<ul><li><code>tf.Variable(initial_value=tf.random_normal(shape=shape, stddev=0.01))</code></li><li><code>stddev</code></li></ul></li><li>选择优化器<ul><li><code>tf.train.AdamOptimizer(0.01).minimize(loss)</code></li></ul></li><li>调整网络<ul><li>使用<code>batch normalization</code>：批标准化</li><li><code>droupout</code>层：使一些神经元失效</li><li>防止过拟合</li></ul></li></ul><h3 id=改为高级-api>改为高级 API</h3><h3 id=主要代码>主要代码</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>create_model</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
    <span class=s2>&#34;&#34;&#34;
</span><span class=s2>    实现构建卷积神经网络
</span><span class=s2>    :return:
</span><span class=s2>    &#34;&#34;&#34;</span>
    <span class=n>y_predict</span> <span class=o>=</span> <span class=mi>0</span>
    <span class=c1># 第一个卷积大层</span>
    <span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>variable_scope</span><span class=p>(</span><span class=s1>&#39;conv1&#39;</span><span class=p>):</span>
        <span class=c1># 卷积层</span>
        <span class=n>input_x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>28</span><span class=p>,</span> <span class=mi>28</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span>

        <span class=n>conv1_weights</span> <span class=o>=</span> <span class=n>create_variable</span><span class=p>([</span><span class=mi>5</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>32</span><span class=p>])</span>
        <span class=n>conv1_bias</span> <span class=o>=</span> <span class=n>create_variable</span><span class=p>([</span><span class=mi>32</span><span class=p>])</span>
        <span class=n>conv1_x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>conv2d</span><span class=p>(</span><span class=n>input_x</span><span class=p>,</span> <span class=nb>filter</span><span class=o>=</span><span class=n>conv1_weights</span><span class=p>,</span> <span class=n>strides</span><span class=o>=</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>padding</span><span class=o>=</span><span class=s1>&#39;SAME&#39;</span><span class=p>)</span> <span class=o>+</span> <span class=n>conv1_bias</span>
        <span class=c1># 激活层</span>
        <span class=n>relu1_x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>conv1_x</span><span class=p>)</span>
        <span class=c1># 池化层</span>
        <span class=n>pool1_x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>max_pool</span><span class=p>(</span><span class=n>relu1_x</span><span class=p>,</span> <span class=n>ksize</span><span class=o>=</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>strides</span><span class=o>=</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>padding</span><span class=o>=</span><span class=s2>&#34;SAME&#34;</span><span class=p>)</span>

    <span class=c1># 第二个卷积大层</span>
    <span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>variable_scope</span><span class=p>(</span><span class=s1>&#39;conv2&#39;</span><span class=p>):</span>
        <span class=c1># 卷积层</span>

        <span class=n>conv2_weights</span> <span class=o>=</span> <span class=n>create_variable</span><span class=p>([</span><span class=mi>5</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=mi>64</span><span class=p>])</span>
        <span class=n>conv2_bias</span> <span class=o>=</span> <span class=n>create_variable</span><span class=p>([</span><span class=mi>64</span><span class=p>])</span>
        <span class=n>conv2_x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>conv2d</span><span class=p>(</span><span class=n>pool1_x</span><span class=p>,</span> <span class=nb>filter</span><span class=o>=</span><span class=n>conv2_weights</span><span class=p>,</span> <span class=n>strides</span><span class=o>=</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>padding</span><span class=o>=</span><span class=s1>&#39;SAME&#39;</span><span class=p>)</span> <span class=o>+</span> <span class=n>conv2_bias</span>

        <span class=c1># 激活层</span>
        <span class=n>relu2_x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>conv2_x</span><span class=p>)</span>

        <span class=c1># 池化层</span>
        <span class=n>pool2_x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>max_pool</span><span class=p>(</span><span class=n>relu2_x</span><span class=p>,</span> <span class=n>ksize</span><span class=o>=</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>strides</span><span class=o>=</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>padding</span><span class=o>=</span><span class=s2>&#34;SAME&#34;</span><span class=p>)</span>
    <span class=c1># 全连接层</span>
    <span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>variable_scope</span><span class=p>(</span><span class=s2>&#34;full_connection&#34;</span><span class=p>):</span>
        <span class=c1># [None, 7, 7, 64]-&gt;[None, 7 * 7 * 64]</span>
        <span class=c1># [None, 7 * 7 * 64] * [7 * 7 * 64, 10] = [None, 10]</span>
        <span class=n>x_fc</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>pool2_x</span><span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>7</span> <span class=o>*</span> <span class=mi>7</span> <span class=o>*</span> <span class=mi>64</span><span class=p>])</span>
        <span class=n>weights_fc</span> <span class=o>=</span> <span class=n>create_variable</span><span class=p>(</span><span class=n>shape</span><span class=o>=</span><span class=p>[</span><span class=mi>7</span> <span class=o>*</span> <span class=mi>7</span> <span class=o>*</span> <span class=mi>64</span><span class=p>,</span> <span class=mi>10</span><span class=p>])</span>
        <span class=n>bias_fc</span> <span class=o>=</span> <span class=n>create_variable</span><span class=p>(</span><span class=n>shape</span><span class=o>=</span><span class=p>[</span><span class=mi>10</span><span class=p>])</span>
        <span class=n>y_predict</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>x_fc</span><span class=p>,</span> <span class=n>weights_fc</span><span class=p>)</span> <span class=o>+</span> <span class=n>bias_fc</span>

    <span class=k>return</span> <span class=n>y_predict</span>
</code></pre></td></tr></table></div></div><h2 id=实战验证码识别>实战：验证码识别</h2><h3 id=数据集>数据集</h3><p>一个图片对应 4 个目标值</p><p><code>NZPP -> [13, 25, 15, 15] -> [[one-hot], [], [], []]</code></p><h3 id=损失衡量>损失衡量</h3><p>softmax 交叉熵，只适用于类别相互排斥的，一个样本对应一个目标值</p><p><code>[4, 26] -> [4*26]</code></p><p>使用 sigmoid 交叉熵</p><ul><li><p>sigmoid 交叉熵损失函数</p></li><li><p><code>tf.nn.sigmoid_cross_entropy_with_logits(labels=None, logits=None)</code></p><ul><li>labels：真实值，为 one_hot 编码形式，和 logits 一样</li><li>logits：logits 值，输出层的加权计算结果</li></ul></li><li><p>对真实值进行 one_hot 编码</p></li><li><p><code>tf.one_hot(indices, depth, axis=None, name=None)</code></p><ul><li>indices：需要编码的张量</li><li>depth：one_hot 编码的深度，这里 26 个字母，为 26</li><li>axis：填充的维度，默认是-1</li></ul></li></ul><h3 id=流程分析>流程分析</h3><ol><li>读取图片数据<ol><li>filename -> 标签值</li></ol></li><li>解析 csv 文件，将标签值转为<code>[字母序号, 字母序号, 字母序号, 字母序号]</code></li><li>将 filename 与标签值联系起来</li><li>构建卷积神经网络<ol><li>利用手写识别网络</li><li>产生 y_predict</li></ol></li><li>构造损失函数</li><li>优化损失</li><li>计算准确率</li><li>开启会话、开启线程</li></ol><h3 id=代码>代码</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>  1
</span><span class=lnt>  2
</span><span class=lnt>  3
</span><span class=lnt>  4
</span><span class=lnt>  5
</span><span class=lnt>  6
</span><span class=lnt>  7
</span><span class=lnt>  8
</span><span class=lnt>  9
</span><span class=lnt> 10
</span><span class=lnt> 11
</span><span class=lnt> 12
</span><span class=lnt> 13
</span><span class=lnt> 14
</span><span class=lnt> 15
</span><span class=lnt> 16
</span><span class=lnt> 17
</span><span class=lnt> 18
</span><span class=lnt> 19
</span><span class=lnt> 20
</span><span class=lnt> 21
</span><span class=lnt> 22
</span><span class=lnt> 23
</span><span class=lnt> 24
</span><span class=lnt> 25
</span><span class=lnt> 26
</span><span class=lnt> 27
</span><span class=lnt> 28
</span><span class=lnt> 29
</span><span class=lnt> 30
</span><span class=lnt> 31
</span><span class=lnt> 32
</span><span class=lnt> 33
</span><span class=lnt> 34
</span><span class=lnt> 35
</span><span class=lnt> 36
</span><span class=lnt> 37
</span><span class=lnt> 38
</span><span class=lnt> 39
</span><span class=lnt> 40
</span><span class=lnt> 41
</span><span class=lnt> 42
</span><span class=lnt> 43
</span><span class=lnt> 44
</span><span class=lnt> 45
</span><span class=lnt> 46
</span><span class=lnt> 47
</span><span class=lnt> 48
</span><span class=lnt> 49
</span><span class=lnt> 50
</span><span class=lnt> 51
</span><span class=lnt> 52
</span><span class=lnt> 53
</span><span class=lnt> 54
</span><span class=lnt> 55
</span><span class=lnt> 56
</span><span class=lnt> 57
</span><span class=lnt> 58
</span><span class=lnt> 59
</span><span class=lnt> 60
</span><span class=lnt> 61
</span><span class=lnt> 62
</span><span class=lnt> 63
</span><span class=lnt> 64
</span><span class=lnt> 65
</span><span class=lnt> 66
</span><span class=lnt> 67
</span><span class=lnt> 68
</span><span class=lnt> 69
</span><span class=lnt> 70
</span><span class=lnt> 71
</span><span class=lnt> 72
</span><span class=lnt> 73
</span><span class=lnt> 74
</span><span class=lnt> 75
</span><span class=lnt> 76
</span><span class=lnt> 77
</span><span class=lnt> 78
</span><span class=lnt> 79
</span><span class=lnt> 80
</span><span class=lnt> 81
</span><span class=lnt> 82
</span><span class=lnt> 83
</span><span class=lnt> 84
</span><span class=lnt> 85
</span><span class=lnt> 86
</span><span class=lnt> 87
</span><span class=lnt> 88
</span><span class=lnt> 89
</span><span class=lnt> 90
</span><span class=lnt> 91
</span><span class=lnt> 92
</span><span class=lnt> 93
</span><span class=lnt> 94
</span><span class=lnt> 95
</span><span class=lnt> 96
</span><span class=lnt> 97
</span><span class=lnt> 98
</span><span class=lnt> 99
</span><span class=lnt>100
</span><span class=lnt>101
</span><span class=lnt>102
</span><span class=lnt>103
</span><span class=lnt>104
</span><span class=lnt>105
</span><span class=lnt>106
</span><span class=lnt>107
</span><span class=lnt>108
</span><span class=lnt>109
</span><span class=lnt>110
</span><span class=lnt>111
</span><span class=lnt>112
</span><span class=lnt>113
</span><span class=lnt>114
</span><span class=lnt>115
</span><span class=lnt>116
</span><span class=lnt>117
</span><span class=lnt>118
</span><span class=lnt>119
</span><span class=lnt>120
</span><span class=lnt>121
</span><span class=lnt>122
</span><span class=lnt>123
</span><span class=lnt>124
</span><span class=lnt>125
</span><span class=lnt>126
</span><span class=lnt>127
</span><span class=lnt>128
</span><span class=lnt>129
</span><span class=lnt>130
</span><span class=lnt>131
</span><span class=lnt>132
</span><span class=lnt>133
</span><span class=lnt>134
</span><span class=lnt>135
</span><span class=lnt>136
</span><span class=lnt>137
</span><span class=lnt>138
</span><span class=lnt>139
</span><span class=lnt>140
</span><span class=lnt>141
</span><span class=lnt>142
</span><span class=lnt>143
</span><span class=lnt>144
</span><span class=lnt>145
</span><span class=lnt>146
</span><span class=lnt>147
</span><span class=lnt>148
</span><span class=lnt>149
</span><span class=lnt>150
</span><span class=lnt>151
</span><span class=lnt>152
</span><span class=lnt>153
</span><span class=lnt>154
</span><span class=lnt>155
</span><span class=lnt>156
</span><span class=lnt>157
</span><span class=lnt>158
</span><span class=lnt>159
</span><span class=lnt>160
</span><span class=lnt>161
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>tensorflow</span> <span class=k>as</span> <span class=nn>tf</span>
<span class=kn>import</span> <span class=nn>glob</span>
<span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
<span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>


<span class=k>def</span> <span class=nf>read_pic</span><span class=p>():</span>
    <span class=s2>&#34;&#34;&#34;
</span><span class=s2>    读取图片数据
</span><span class=s2>    :return:
</span><span class=s2>    &#34;&#34;&#34;</span>
    <span class=c1># 1、文件队列</span>
    <span class=n>file_names</span> <span class=o>=</span> <span class=n>glob</span><span class=o>.</span><span class=n>glob</span><span class=p>(</span><span class=s1>&#39;./GenPics/*.jpg&#39;</span><span class=p>)</span>
    <span class=n>file_queue</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>train</span><span class=o>.</span><span class=n>string_input_producer</span><span class=p>(</span><span class=n>file_names</span><span class=p>)</span>
    <span class=c1># 2、读取与解码</span>
    <span class=n>reader</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>WholeFileReader</span><span class=p>()</span>
    <span class=n>file_name</span><span class=p>,</span> <span class=n>image</span> <span class=o>=</span> <span class=n>reader</span><span class=o>.</span><span class=n>read</span><span class=p>(</span><span class=n>file_queue</span><span class=p>)</span>
    <span class=n>decoded</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>image</span><span class=o>.</span><span class=n>decode_jpeg</span><span class=p>(</span><span class=n>image</span><span class=p>)</span>
    <span class=c1># 更新形状，将图片形状确定下来</span>
    <span class=n>decoded</span><span class=o>.</span><span class=n>set_shape</span><span class=p>([</span><span class=mi>20</span><span class=p>,</span> <span class=mi>80</span><span class=p>,</span> <span class=mi>3</span><span class=p>])</span>

    <span class=c1># 修改图片类型</span>
    <span class=n>image_cast</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>cast</span><span class=p>(</span><span class=n>decoded</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
    <span class=c1># 3、批处理</span>
    <span class=n>filename_batch</span><span class=p>,</span> <span class=n>image_batch</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>train</span><span class=o>.</span><span class=n>batch</span><span class=p>([</span><span class=n>file_name</span><span class=p>,</span> <span class=n>image_cast</span><span class=p>],</span> <span class=n>batch_size</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>num_threads</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>capacity</span><span class=o>=</span><span class=mi>200</span><span class=p>)</span>
    <span class=k>return</span> <span class=n>filename_batch</span><span class=p>,</span> <span class=n>image_batch</span>


<span class=k>def</span> <span class=nf>parse_csv</span><span class=p>():</span>
    <span class=s2>&#34;&#34;&#34;
</span><span class=s2>    解析csv文件，建立文件名与标签值表格
</span><span class=s2>    :return:
</span><span class=s2>    &#34;&#34;&#34;</span>
    <span class=n>csv_data</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s2>&#34;./GenPics/labels.csv&#34;</span><span class=p>,</span> <span class=n>names</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;file_num&#34;</span><span class=p>,</span> <span class=s2>&#34;chars&#34;</span><span class=p>],</span> <span class=n>index_col</span><span class=o>=</span><span class=s2>&#34;file_num&#34;</span><span class=p>)</span>

    <span class=n>labels</span> <span class=o>=</span> <span class=p>[]</span>
    <span class=k>for</span> <span class=n>label</span> <span class=ow>in</span> <span class=n>csv_data</span><span class=p>[</span><span class=s2>&#34;chars&#34;</span><span class=p>]:</span>
        <span class=n>tmp</span> <span class=o>=</span> <span class=p>[]</span>
        <span class=k>for</span> <span class=n>letter</span> <span class=ow>in</span> <span class=n>label</span><span class=p>:</span>
            <span class=n>tmp</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=nb>ord</span><span class=p>(</span><span class=n>letter</span><span class=p>)</span> <span class=o>-</span> <span class=nb>ord</span><span class=p>(</span><span class=s2>&#34;A&#34;</span><span class=p>))</span>
        <span class=n>labels</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>tmp</span><span class=p>)</span>

    <span class=n>csv_data</span><span class=p>[</span><span class=s2>&#34;labels&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>labels</span>

    <span class=k>return</span> <span class=n>csv_data</span>


<span class=k>def</span> <span class=nf>filename2label</span><span class=p>(</span><span class=n>filenames</span><span class=p>,</span> <span class=n>csv_data</span><span class=p>):</span>
    <span class=s2>&#34;&#34;&#34;
</span><span class=s2>    将样本特征值与目标值一一对应
</span><span class=s2>    :param filenames:
</span><span class=s2>    :param csv_data:
</span><span class=s2>    :return:
</span><span class=s2>    &#34;&#34;&#34;</span>
    <span class=n>labels</span> <span class=o>=</span> <span class=p>[]</span>

    <span class=c1># 将b&#39;文件名中的数字提取出来并索引相应的标签值</span>

    <span class=k>for</span> <span class=n>file_name</span> <span class=ow>in</span> <span class=n>filenames</span><span class=p>:</span>
        <span class=n>digit_str</span> <span class=o>=</span> <span class=s2>&#34;&#34;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=nb>list</span><span class=p>(</span><span class=nb>filter</span><span class=p>(</span><span class=nb>str</span><span class=o>.</span><span class=n>isdigit</span><span class=p>,</span> <span class=nb>str</span><span class=p>(</span><span class=n>file_name</span><span class=p>))))</span>
        <span class=n>label</span> <span class=o>=</span> <span class=n>csv_data</span><span class=o>.</span><span class=n>loc</span><span class=p>[</span><span class=nb>int</span><span class=p>(</span><span class=n>digit_str</span><span class=p>),</span> <span class=s2>&#34;labels&#34;</span><span class=p>]</span>
        <span class=n>labels</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>label</span><span class=p>)</span>
    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>labels</span><span class=p>)</span>


<span class=k>def</span> <span class=nf>create_variable</span><span class=p>(</span><span class=n>shape</span><span class=p>):</span>
    <span class=s2>&#34;&#34;&#34;
</span><span class=s2>    创建变量
</span><span class=s2>    :param shape:
</span><span class=s2>    :return:
</span><span class=s2>    &#34;&#34;&#34;</span>
    <span class=k>return</span> <span class=n>tf</span><span class=o>.</span><span class=n>Variable</span><span class=p>(</span><span class=n>initial_value</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>random_normal</span><span class=p>(</span><span class=n>shape</span><span class=o>=</span><span class=n>shape</span><span class=p>,</span> <span class=n>stddev</span><span class=o>=</span><span class=mf>0.01</span><span class=p>))</span>


<span class=k>def</span> <span class=nf>create_model</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
    <span class=s2>&#34;&#34;&#34;
</span><span class=s2>    实现构建卷积神经网络
</span><span class=s2>    :param x:[None, 20, 80, 3]
</span><span class=s2>    :return:
</span><span class=s2>    &#34;&#34;&#34;</span>
    <span class=c1># 第一个卷积大层</span>
    <span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>variable_scope</span><span class=p>(</span><span class=s1>&#39;conv1&#39;</span><span class=p>):</span>
        <span class=c1># 卷积层</span>
        <span class=n>conv1_weights</span> <span class=o>=</span> <span class=n>create_variable</span><span class=p>([</span><span class=mi>5</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>32</span><span class=p>])</span>
        <span class=n>conv1_bias</span> <span class=o>=</span> <span class=n>create_variable</span><span class=p>([</span><span class=mi>32</span><span class=p>])</span>
        <span class=n>conv1_x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>conv2d</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=nb>filter</span><span class=o>=</span><span class=n>conv1_weights</span><span class=p>,</span> <span class=n>strides</span><span class=o>=</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>padding</span><span class=o>=</span><span class=s1>&#39;SAME&#39;</span><span class=p>)</span> <span class=o>+</span> <span class=n>conv1_bias</span>
        <span class=c1># 激活层</span>
        <span class=n>relu1_x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>conv1_x</span><span class=p>)</span>
        <span class=c1># 池化层</span>
        <span class=n>pool1_x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>max_pool</span><span class=p>(</span><span class=n>relu1_x</span><span class=p>,</span> <span class=n>ksize</span><span class=o>=</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>strides</span><span class=o>=</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>padding</span><span class=o>=</span><span class=s2>&#34;SAME&#34;</span><span class=p>)</span>

    <span class=c1># 第二个卷积大层</span>
    <span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>variable_scope</span><span class=p>(</span><span class=s1>&#39;conv2&#39;</span><span class=p>):</span>
        <span class=c1># [None, 20, 80, 3] --&gt; [None, 10, 40, 32]</span>
        <span class=c1># 卷积层</span>
        <span class=n>conv2_weights</span> <span class=o>=</span> <span class=n>create_variable</span><span class=p>([</span><span class=mi>5</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=mi>64</span><span class=p>])</span>
        <span class=n>conv2_bias</span> <span class=o>=</span> <span class=n>create_variable</span><span class=p>([</span><span class=mi>64</span><span class=p>])</span>
        <span class=n>conv2_x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>conv2d</span><span class=p>(</span><span class=n>pool1_x</span><span class=p>,</span> <span class=nb>filter</span><span class=o>=</span><span class=n>conv2_weights</span><span class=p>,</span> <span class=n>strides</span><span class=o>=</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>padding</span><span class=o>=</span><span class=s1>&#39;SAME&#39;</span><span class=p>)</span> <span class=o>+</span> <span class=n>conv2_bias</span>
        <span class=c1># 激活层</span>
        <span class=n>relu2_x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>conv2_x</span><span class=p>)</span>
        <span class=c1># 池化层</span>
        <span class=n>pool2_x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>max_pool</span><span class=p>(</span><span class=n>relu2_x</span><span class=p>,</span> <span class=n>ksize</span><span class=o>=</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>strides</span><span class=o>=</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>padding</span><span class=o>=</span><span class=s2>&#34;SAME&#34;</span><span class=p>)</span>
    <span class=c1># 全连接层</span>
    <span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>variable_scope</span><span class=p>(</span><span class=s2>&#34;full_connection&#34;</span><span class=p>):</span>
        <span class=c1># [None, 10, 40, 32] -&gt; [None, 5, 20, 64]</span>
        <span class=c1># [None, 5, 20, 64] -&gt; [None, 5 * 20 * 64]</span>
        <span class=c1># [None, 5 * 20 * 64] * [5 * 20 * 64, 4 * 26] = [None, 4 * 26]</span>
        <span class=n>x_fc</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>pool2_x</span><span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>5</span> <span class=o>*</span> <span class=mi>20</span> <span class=o>*</span> <span class=mi>64</span><span class=p>])</span>
        <span class=n>weights_fc</span> <span class=o>=</span> <span class=n>create_variable</span><span class=p>(</span><span class=n>shape</span><span class=o>=</span><span class=p>[</span><span class=mi>5</span> <span class=o>*</span> <span class=mi>20</span> <span class=o>*</span> <span class=mi>64</span><span class=p>,</span> <span class=mi>4</span> <span class=o>*</span> <span class=mi>26</span><span class=p>])</span>
        <span class=n>bias_fc</span> <span class=o>=</span> <span class=n>create_variable</span><span class=p>(</span><span class=n>shape</span><span class=o>=</span><span class=p>[</span><span class=mi>104</span><span class=p>])</span>
        <span class=n>y_predict</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>x_fc</span><span class=p>,</span> <span class=n>weights_fc</span><span class=p>)</span> <span class=o>+</span> <span class=n>bias_fc</span>

    <span class=k>return</span> <span class=n>y_predict</span>


<span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s1>&#39;__main__&#39;</span><span class=p>:</span>
    <span class=n>filename</span><span class=p>,</span> <span class=n>image</span> <span class=o>=</span> <span class=n>read_pic</span><span class=p>()</span>
    <span class=n>csv_data</span> <span class=o>=</span> <span class=n>parse_csv</span><span class=p>()</span>

    <span class=c1># 1、准备数据</span>
    <span class=n>x</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>placeholder</span><span class=p>(</span><span class=n>dtype</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>[</span><span class=kc>None</span><span class=p>,</span> <span class=mi>20</span><span class=p>,</span> <span class=mi>80</span><span class=p>,</span> <span class=mi>3</span><span class=p>])</span>
    <span class=n>y_true</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>placeholder</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>[</span><span class=kc>None</span><span class=p>,</span> <span class=mi>4</span> <span class=o>*</span> <span class=mi>26</span><span class=p>])</span>

    <span class=c1># 2、构建模型</span>
    <span class=n>y_predict</span> <span class=o>=</span> <span class=n>create_model</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>

    <span class=c1># 3、构造损失函数</span>
    <span class=n>loss_list</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>sigmoid_cross_entropy_with_logits</span><span class=p>(</span><span class=n>labels</span><span class=o>=</span><span class=n>y_true</span><span class=p>,</span> <span class=n>logits</span><span class=o>=</span><span class=n>y_predict</span><span class=p>)</span>
    <span class=n>loss</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_mean</span><span class=p>(</span><span class=n>loss_list</span><span class=p>)</span>

    <span class=c1># 4、优化损失</span>
    <span class=n>optimizer</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>train</span><span class=o>.</span><span class=n>AdamOptimizer</span><span class=p>(</span><span class=n>learning_rate</span><span class=o>=</span><span class=mf>0.001</span><span class=p>)</span><span class=o>.</span><span class=n>minimize</span><span class=p>(</span><span class=n>loss</span><span class=p>)</span>
    <span class=c1># optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)</span>

    <span class=c1># 5、计算准确率</span>
    <span class=n>y_predict_argmax</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>y_predict</span><span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>26</span><span class=p>]),</span> <span class=n>axis</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
    <span class=n>y_true_argmax</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>26</span><span class=p>]),</span> <span class=n>axis</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
    <span class=n>equal</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>equal</span><span class=p>(</span><span class=n>y_predict_argmax</span><span class=p>,</span> <span class=n>y_true_argmax</span><span class=p>)</span>
    <span class=n>equal_list</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_all</span><span class=p>(</span><span class=n>equal</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
    <span class=n>accuracy</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_mean</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>cast</span><span class=p>(</span><span class=n>equal_list</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>))</span>

    <span class=c1># 初始化变量</span>
    <span class=n>init</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>global_variables_initializer</span><span class=p>()</span>

    <span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>Session</span><span class=p>()</span> <span class=k>as</span> <span class=n>sess</span><span class=p>:</span>
        <span class=n>sess</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>init</span><span class=p>)</span>
        <span class=n>coord</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>train</span><span class=o>.</span><span class=n>Coordinator</span><span class=p>()</span>
        <span class=n>threads</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>train</span><span class=o>.</span><span class=n>start_queue_runners</span><span class=p>(</span><span class=n>sess</span><span class=o>=</span><span class=n>sess</span><span class=p>,</span> <span class=n>coord</span><span class=o>=</span><span class=n>coord</span><span class=p>)</span>

        <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>1000</span><span class=p>):</span>
            <span class=n>filename_value</span><span class=p>,</span> <span class=n>image_value</span> <span class=o>=</span> <span class=n>sess</span><span class=o>.</span><span class=n>run</span><span class=p>([</span><span class=n>filename</span><span class=p>,</span> <span class=n>image</span><span class=p>])</span>
            <span class=n>labels</span> <span class=o>=</span> <span class=n>filename2label</span><span class=p>(</span><span class=n>filename_value</span><span class=p>,</span> <span class=n>csv_data</span><span class=p>)</span>
            <span class=c1># 标签值转换为one-hot</span>
            <span class=n>labels_value</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>one_hot</span><span class=p>(</span><span class=n>labels</span><span class=p>,</span> <span class=n>depth</span><span class=o>=</span><span class=mi>26</span><span class=p>),</span> <span class=n>shape</span><span class=o>=</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>4</span> <span class=o>*</span> <span class=mi>26</span><span class=p>])</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
            <span class=n>_</span><span class=p>,</span> <span class=n>error</span><span class=p>,</span> <span class=n>accuracy_value</span> <span class=o>=</span> <span class=n>sess</span><span class=o>.</span><span class=n>run</span><span class=p>([</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>loss</span><span class=p>,</span> <span class=n>accuracy</span><span class=p>],</span>
                                                <span class=n>feed_dict</span><span class=o>=</span><span class=p>{</span><span class=n>x</span><span class=p>:</span> <span class=n>image_value</span><span class=p>,</span> <span class=n>y_true</span><span class=p>:</span> <span class=n>labels_value</span><span class=p>})</span>
            <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;第</span><span class=si>%d</span><span class=s2>次训练后损失为</span><span class=si>%f</span><span class=s2>，准确率为</span><span class=si>%f</span><span class=s2>&#34;</span> <span class=o>%</span> <span class=p>(</span><span class=n>i</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span> <span class=n>error</span><span class=p>,</span> <span class=n>accuracy_value</span><span class=p>))</span>

        <span class=n>coord</span><span class=o>.</span><span class=n>request_stop</span><span class=p>()</span>
        <span class=n>coord</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>threads</span><span class=p>)</span>

</code></pre></td></tr></table></div></div></div><div class=post-copyright><p class=copyright-item><span class=item-title>Author</span>
<span class=item-content>zyuanlee</span></p><p class=copyright-item><span class=item-title>LastMod</span>
<span class=item-content>2020-03-19</span></p></div><footer class=post-footer><div class=post-tags><a href=/tags/tensorflow/>TensorFlow</a>
<a href=/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>深度学习</a></div><nav class=post-nav><a class=prev href=/post/2020-05-02_%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BDa%E7%90%86%E8%AE%BA%E7%9F%A5%E8%AF%86%E7%82%B9/><i class="iconfont icon-left"></i>
<span class="prev-text nav-default">人工智能A理论知识点</span>
<span class="prev-text nav-mobile">Prev</span></a>
<a class=next href=/post/2020-03-14_tensorflow%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/><span class="next-text nav-default">TensorFlow数据读取、神经网络</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i></a></nav></footer></article></div></div></main><footer id=footer class=footer><div class=social-links><a href=zyuanlee97@gmail.com class="iconfont icon-email" title=email></a><a href=https://github.com/Pandalzy class="iconfont icon-github" title=github></a><a href=http://localhost:1313/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a></span>
<span class=copyright-year>&copy;
2019 -
2021<span class=heart><i class="iconfont icon-heart"></i></span><span>zyuanlee</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script><script type=text/javascript src=/js/main.min.c99b103c33d1539acf3025e1913697534542c4a5aa5af0ccc20475ed2863603b.js></script></body></html>